# Related work

## Data Manipulation in Analytical Systems

When people build systems that analyze large volumes of data, it is rarely enough to simply “collect and store” the data. You usually need a full pipeline that extracts raw inputs, cleans or transforms them, and then loads them into a database or analysis system. This process, often called ETL (extract-transform-load, or in modern contexts, variant pipelines) ensures that data is accurate, consistent, and in the right format for analysis. But recent research shows that these pipelines are often fragile and have a good amount of errors. In a large study, engineers identified 41 different factors that influence whether a data pipeline produces high-quality data, such as the variety of data sources, the complexity of transformations, mismatch of data types, and insufficient checks during cleaning and ingestion. [@DataPipelineQuality2023]

Many of the data problems tend to happen early on especially during the “cleaning” stage, when raw inputs are standardized, and during “ingestion/integration,” when data from different sources are combined. [@DataPipelineQuality2023]  If a pipeline is not carefully designed with validation steps, schema definitions (clear rules for what fields and data types are expected), and metadata or provenance tracking (records of where each piece of data came from and how it was transformed) then errors can silently corrupt the dataset. This is why transparency plays a major concept of DataSeekr.

For sports data which can include traditional box score stats, lists of matches, roster databases, video metadata, wearable sensor outputs, etc. this fragility is especially concerning. The mixture of structured, semi-structured, and unstructured data means that schema mismatches, missing fields, or mistyped data are likely, and without proper pipeline safeguards, derived metrics (like “player performance” or “projectability”) risk being misleading. A system like DataSeekr that defines a stable ingestion schema, stores raw as well as transformed data, and logs transformations offers a more reliable foundation for analytics than scraping and manual manipulation.

## Sports Analytics and Athlete-Evaluation Technologies

In recent years, the field of sports analytics has expanded far beyond traditional box scores and basic statistics. Researchers and practitioners increasingly use wearable sensors (like GPS trackers or accelerometers), motion data, and computer vision algorithms on video to analyze biomechanical performance, movement patterns, positional data, and even predict injuries or long term workload. [@AIWearables2024] This is especially true for higher division and higher funded schools as they measure more as they have the technology. However this is more for training purposes only.

Moreover, machine learning (ML) and artificial intelligence (AI) methods are being applied to these data: deep learning models, for example, it can process sensor signals or video frames to automatically detect events (a sprint, a jump, a shot), track trajectories, or talk about workload and stress patterns. [@WearableSportsReview2024 ] This promises to give much richer, more nuanced insight into athletic performance than classical statistics for example, quantifying not just how many hits or hits per game, but movement efficiency, reaction times, recovery patterns, or biomechanical risk factors. However these same technologies aren't offered or are too expensive for lower division schools.

However, despite the promise, there remain serious limitations. Many studies have small sample sizes, sometimes only lab based trials, or focus on a narrow subset of athletes under controlled conditions. That makes it hard to know whether conclusions generalize to real-world, competitive settings or different populations. [@AIWearables2024]  Also, variability in sensor hardware, sampling frequency, calibration protocols, and preprocessing methods across studies means combining or comparing results across different teams or contexts is very difficult. For instance, what “acceleration pattern X” means in one study might be incompatible with another’s data because of different sensor settings or cleaning methods. [@DeepLearningAthlete2023] Thus calling for a centralized and managed output that can be implemented into their own work.

Because of these issues reproducibility, generalizability, and high technical access cost, advanced wearables/vision-based analytics remain largely confined to elite or well-funded sports organizations. Many lower-division or less resourced programs (high school, small colleges, semi-pro leagues) simply cannot adopt them, which perpetuates inequality in access.

In contrast, a tool like DataSeekr that relies on publicly available data (e.g. game stats, official records) rather than specialized sensors or advanced hardware can be broadly used. While it sacrifices some of the rich detail that wearable/vision systems offer, it gains in accessibility, scalability, and fairness allowing more athletes to benefit, regardless of resources.

## Web Scraping and Automated Data Collection

“Web scraping” means using automated software (bots) to retrieve data from web pages, parse relevant information, and store it in a structured form (e.g. CSV, database). For sports analytics, web scraping is often an attractive way to collect publicly available statistics, game logs, roster listings, highlight-video metadata, schedules, and more especially when no official API or bulk-export feature exists.

But scraping comes with serious ethical, legal, and technical considerations. Even if the data are public in principle, a website’s terms of service may forbid automated scraping or bulk harvesting. There are also potential copyright or database right issues, especially if the site collects and compiles data. According to recent research, as scraping becomes more widespread (for example to feed large language models or AI training sets), many site owners are tightening policies, making scraping riskier. [@EthicsScraping2024]

Beyond legal risks, scraping can impose real burdens on websites. If many bots crawl a site rapidly, it can degrade performance, cause partial outages, or trigger blocking. Ethical guidelines for responsible scraping recommend respecting rate-limits (delays between requests), obeying robots exclusion rules (robots.txt), avoiding repeated heavy access, and documenting provenance (where the data came from, when it was collected). [@EthicsScraping2024]

For DataSeekr, building a scraping-based ingestion system means you must embed these ethical/legal safeguards. By limiting collection to publicly available data, avoiding redistribution of copyrighted content, documenting provenance, and respecting site load, you reduce legal and ethical risk. That makes your system far more defensible than a “scrape first, ask forgiveness later” approach.

## Limitations of Existing Athlete Platforms

Many existing athlete platforms, whether they host highlight videos, recruiting profiles, or analytics dashboards offer useful services for players, coaches, and scouts. However, these platforms have structural limitations that make them imperfect tools for fair and broad talent discovery. These sites might give recommendations which is good but also creates an imbalance or a sense of unfairness.

Platforms often suffer from data lock-in. Once a player uploads their video or stats to that platform, exporting them in bulk, or integrating them with independent tools, can be difficult or forbidden by the platform’s terms. This limits portability and makes independent validation or custom analysis hard. This more impacts the player as this can reduce a bunch of opportunities for them as some prices which i have discovered first hand. This is because I am an athlete, I used a service that had a super high payment cost which really deferred other options because I didn't have that many funds. This is just one real world example of this happening. 

There is a significant visibility bias. Players from well resourced or high visibility programs tend to get more filming, better video quality, more highlight clips  which increases their exposure by default. Meanwhile, players from smaller schools or lower divisions may perform just as well but lack the video, the editors, or the promotion to catch scouts’ eyes. This creates structural inequality in exposure opportunities. To think of this, imagine a player at Allegheny, no guarantee to be recorded and the event mostly likely won't be on television, whereas someone at LSU would have their games being broadcasted to millions of people. This creates a major bias within the recruitment world.

Many platforms provide opaque or proprietary metrics. They might display a “player grade”,  “efficiency rating”,  or “projectability score” but without exposing raw data or the algorithm behind them. Without transparency, it’s impossible for coaches or third-party analysts to verify or challenge those scores, compare across platforms, or adapt them to different contexts. Because of these limitations, many talented athletes slip through the cracks simply because they don’t have the right profile on the “right” platform. A system like DataSeekr which aggregates publicly available data, provides raw and derived metrics in a transparent, uniform schema, and avoids reliance on platform specific video uploads offers an alternative. By leveling the “exposure playing field,” it helps reduce bias and gives scouts a more equitable view of players from all backgrounds.

## Gaps in Current Research and How DataSeekr Contributes

Even with all the advances in sports analytics, data pipelines, and automated evaluation, there remain several persistent gaps. First, there is a lack of standardized, widely accessible datasets or schema designs in sports. Because sensor based studies, video analytics projects, and manual box score scraping all use different formats, it’s very difficult to combine or compare data across teams, studies, or time periods. Without standardization, analysis, reproducible research remains unlikely.

On the engineering side, many academic or applied projects treat data ingestion as a one time task rather than building robust, maintainable pipelines. That means over time the data may become inconsistent, corrupted, or lose provenance; especially in longitudinal studies or when scaling to many players or seasons.

Despite the growing discussion around ethics and legality, many sports-analytics projects do not sufficiently document how they collect, store, and share data; nor do they always consider compliance with site terms, data rights, or privacy implications when combining datasets (e.g. pairing performance data with personal data).

By creating DataSeekr, I made a system that uses publicly available data, builds a transparent ingestion pipeline (scrape → CSV → database → visualization), and exports data in a uniform, documented schema. It will address all three gaps. DataSeekr can serve as a more democratic, standardized, and repeatable foundation for athlete analytics. It reduces dependence on privileged resources (like expensive sensors), lowers barriers, and supports fairer exposure for athletes regardless of background.

In doing so, DataSeekr could help broaden access to sports data analytics, enable longitudinal studies across many players or seasons, and encourage more open, reproducible, and equitable research or scouting practices.
