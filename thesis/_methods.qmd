# Methods 

## Overview 

To address the issues stated around player stats accessibility, I built DataSeekr. Which is a tool that makes player stats more accessible and easy to find. The tool is a search engine that gives visuals along with the player stats and on the backend there is a scraper updating the database that the searching function accesses. The workflow looks like this :

			
![FlowChart](images/WorkFlow.png)


## Grabber

	Looking at the flow chart we see that this function is where it all starts. To begin,my grabber function handles a few things. Things such as making a csv, making that csv a database. I have it this way because csv files are easily manipulated, promoting a more streamlined process as anyone who wants to make their own adaptations can take the csv. 
	
For the actual scraping portion we have two main actors that really make all this possible are selenium and Beautifulsoup. I chose Selenium for a list of reasons, however the main reason is that selenium can handle javascript. This matters because the normal standard scrapers (ex. requests) can't always run javascript, leading to the site not actually being rendered leaving an empty pull from the site. This becomes an issue if you are using a scraper that can't handle javascript scrape but you would end up the table creation in the database, however no information will be stored or you get an error code. For my tool it loads the entire site and brings it to me, selenium ensures page content has fully rendered before trying anything else. Here is where beautiful comes into play as it uses the fully rendered site created from selenium to parse the HTML code. I have also implemented time restraints of five seconds to not run into bot protections for rapid scraping.BeautifulSoup, after parsing the HTML code, follows the path. Here is an example of a path/information in HTML code within PAC site:

					
				
![HTML Info](images/HTML.png)
