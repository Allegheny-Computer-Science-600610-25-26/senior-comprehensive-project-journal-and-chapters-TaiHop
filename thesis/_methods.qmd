# Methods 


## Overview 

To address the issues stated around player stats accessibility, I built a tool named DataSeekr. Which is a tool that makes player stats more accessible and easy to find. The tool is a search engine that gives visuals along with the player stats and on the backend there is a scraper updating the database that the searching function accesses. The workflow looks like this :


			
![FlowChart](images/WorkFlow.png)


The versions for everything inside DataSeekr are as Follows:

- Python --> 3.11
- matplotlib --> 3.10.7
- plotly --> 6.3.1
- streamlit --> 1.52.2
- selenium --> 4.18
- beautifulsoup4 --> 4.12
- webdriver-manager --> 4.0
- pandas --> 2.2


## Grabber

Looking at the flow chart, this is where the entire process begins. The grabber function serves as the foundation of the system and is responsible for several essential tasks. These include generating a CSV file from the scraped data and then converting that CSV into a database. This approach was chosen because CSV files are easy to manipulate, review, and share, which promotes a more streamlined and flexible workflow. By using CSV files as an intermediate step, anyone who wants to make their own adaptations or extensions to the project can work directly with the CSV data without needing to interact with the database layer.

For the scraping portion of the project, two primary tools make the process possible: Selenium and BeautifulSoup. Selenium was selected for multiple reasons, with the most important being its ability to handle JavaScript-driven content. Many modern websites rely heavily on JavaScript to load data dynamically, and standard scraping tools such as requests are often unable to render this content properly. When JavaScript is not executed correctly, the page may appear empty, which can lead to situations where database tables are created but contain no data, or where scraping attempts result in errors. Selenium avoids these issues by fully loading and rendering the entire webpage before any data extraction occurs, ensuring that all required content is available.

Once the webpage has been fully rendered by Selenium, BeautifulSoup is used to parse the resulting HTML. BeautifulSoup navigates the HTML structure and follows defined paths to locate and extract the necessary information. To ensure reliable scraping and avoid triggering bot protection mechanisms, a five-second delay is implemented between scraping actions. This delay allows pages to load completely and helps maintain responsible scraping behavior. Below is an example of how the HTML structure and data paths appear within the PAC site:



![HTML Info](images/HTML.png)



After the data paths are identified within the HTML structure, the system collects a predefined set of key statistics for hitting, pitching, and fielding. These metrics were selected in advance because they are widely used to evaluate player performance and directly support the goals of the project. By limiting the scope of the data to these essential statistics, the dataset remains clean and focused, making it easier to analyze trends and compare players across different teams and seasons.

To ensure consistency throughout the dataset, all statistics are renamed from the labels used on the source website to standardized column names within the database. This prevents inconsistencies and guarantees that each statistic is always stored under a uniform identifier. Each player is represented by a Player object that stores the player’s name, school, season, and associated statistics. This objectoriented design keeps the data well organized and allows the project to be expanded or modified easily in the future.

The collected data is initially saved into CSV files before being loaded into a local database. Using CSV files as an intermediate step makes the data easier to inspect, debug, and reuse if issues arise during the database loading process. This design also allows the database to be rebuilt without rescraping the website, saving time and reducing unnecessary requests to the source site.

Finally, the program incorporates intentional delays between page loads and team scrapes to ensure that content fully renders and to minimize strain on the website’s servers. These delays improve the reliability and accuracy of the data collection process while following responsible web scraping practices.



## DB Maker (Code.py)

As we discussed how the csv transforms into a database, this section will handle any questions regarding that topic. To begin things we need to get a good look at the concept or map of the steps of the conversion process. Below is an image of the map of converter: 


![Csv to DB Concept](images/Converter_Workflow.png)


This data ingestion process reliably loads statistical data from CSV files into a centralized SQLite database while keeping data safe and easy to manage. It starts by reading the CSV into a pandas DataFrame because pandas makes it simple to parse, inspect, and transform tabular data. Using pandas also means I can handle many CSV formats and perform validation and cleaning before anything is written to the database.

Normalization of  all column names to lowercase to keep the database schema consistent. Small differences in capitalization can cause hard to find errors when querying or joining data. By forcing column names to a single case I reduce mismatch problems and make downstream queries and code simpler and more predictable. Then, I  validate that a year column exists because the year is the primary way I identify which records belong together in time. Without a year it becomes difficult to manage historical data or safely replace a single season of data. Requiring the year early lets me detect malformed files before any database changes are made.

I used SQLite for the master database because it is lightweight, easy to set up, and works well for many small to medium sized data projects. It requires no separate server which makes development and deployment simpler. The same design could be used with a client server database later if the project grows. I made section specific tables such as hitting stats or pitching stats when needed and I built the table schema dynamically from the CSV columns. Dynamically creating columns lets the database adapt to schema changes in the CSV without manual SQL changes. I also add an auto incrementing id primary key to give each row a stable unique identifier which helps with traceability and debugging.

Before inserting new data I identify all the year and school combinations present in the CSV and delete only existing rows that match those combinations. I do this because it avoids wiping the whole table and prevents accidental loss of unrelated data. Targeted deletion lets me reimport a file safely without creating duplicate rows or disturbing other schools or other years. If the row wasn't replaced then I append the new CSV rows to the table inside a transaction and then commit the transaction. Using transactions ensures that the database never ends up in a partial state if something goes wrong during the load. I close the database connection when the work is done to free resources. I also log a success message that states how many rows were loaded so there is an auditable record of the operation and simple feedback for the user.

In summary I chose pandas for flexible parsing and transformation, lowercase column names for schema consistency, year validation for correct identity, dual method school detection for flexibility, SQLite for simplicity, dynamic schema creation for adaptability, targeted deletion for safety, creating a safe and reliable tool.


## Search / UI (Seekr.py)

