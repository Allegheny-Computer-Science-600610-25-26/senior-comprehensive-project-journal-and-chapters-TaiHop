# Methods 


## Overview 

To address the issues stated around player stats accessibility, I built a tool named DataSeekr. Which is a tool that makes player stats more accessible and easy to find. The tool is a search engine that gives visuals along with the player stats and on the backend there is a scraper updating the database that the searching function accesses. The workflow looks like this :

			
![FlowChart](images/WorkFlow.png)


## Grabber

Looking at the flow chart, this is where the entire process begins. The grabber function serves as the foundation of the system and is responsible for several essential tasks. These include generating a CSV file from the scraped data and then converting that CSV into a database. This approach was chosen because CSV files are easy to manipulate, review, and share, which promotes a more streamlined and flexible workflow. By using CSV files as an intermediate step, anyone who wants to make their own adaptations or extensions to the project can work directly with the CSV data without needing to interact with the database layer.

For the scraping portion of the project, two primary tools make the process possible: Selenium and BeautifulSoup. Selenium was selected for multiple reasons, with the most important being its ability to handle JavaScript-driven content. Many modern websites rely heavily on JavaScript to load data dynamically, and standard scraping tools such as requests are often unable to render this content properly. When JavaScript is not executed correctly, the page may appear empty, which can lead to situations where database tables are created but contain no data, or where scraping attempts result in errors. Selenium avoids these issues by fully loading and rendering the entire webpage before any data extraction occurs, ensuring that all required content is available.

Once the webpage has been fully rendered by Selenium, BeautifulSoup is used to parse the resulting HTML. BeautifulSoup navigates the HTML structure and follows defined paths to locate and extract the necessary information. To ensure reliable scraping and avoid triggering bot protection mechanisms, a five-second delay is implemented between scraping actions. This delay allows pages to load completely and helps maintain responsible scraping behavior. Below is an example of how the HTML structure and data paths appear within the PAC site:



![HTML Info](images/HTML.png)
